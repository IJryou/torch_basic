{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.5.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python35264bitf8c6bb909d074297b47234dde81826f2",
   "display_name": "Python 3.5.2 64-bit"
  }
 },
 "cells": [
  {
   "source": [
    "import torch"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 2
  },
  {
   "source": [
    "# define activation function\n",
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 3
  },
  {
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 5 random normal variables\n",
    "features = torch.randn((1, 5))\n",
    "# True weights for our data, random normal variables again\n",
    "weights = torch.randn_like(features)\n",
    "# and a true bias term\n",
    "bias = torch.randn((1, 1))"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 4
  },
  {
   "source": [
    "# Calculate output\n",
    "# np.dot <=> torch.matmul or torch.mm\n",
    "# for transpose => object.reshape(a,b) : return new tensor, as in it copies data to another part of memory\n",
    "# for transpose => object.resize_(a,b) : returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory).\n",
    "# Here I should note that the underscore at the end of the method denotes that this method is performed in-place.\n",
    "# for transpose => object.view(a,b) : will return a new tensor with the same data as weights with size (a,b)\n",
    "# https://discuss.pytorch.org/t/what-is-in-place-operation/16244\n",
    "\n",
    "# using sum\n",
    "print(activation((features * weights).sum() + bias))\n",
    "# using torch.mm\n",
    "print(activation(torch.mm(features, weights.view(5,1)) + bias))"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.1595]])\ntensor([[0.1595]])\n"
    }
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 8
  },
  {
   "source": [
    "value_ith = activation(torch.mm(features, W1) + B1)\n",
    "res_hto = activation(torch.mm(value_ith, W2) + B2)\n",
    "res_hto"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.3171]])"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {},
   "execution_count": 15
  },
  {
   "source": [
    "# numpy - torch.tensor\n",
    "import numpy as np\n",
    "a = np.random.rand(4,3)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 17
  },
  {
   "source": [
    "b = torch.from_numpy(a)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 19
  },
  {
   "source": [
    "b"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.3253, 0.7865, 0.7163],\n        [0.5673, 0.8860, 0.4592],\n        [0.2449, 0.6925, 0.2486],\n        [0.6354, 0.5284, 0.9344]], dtype=torch.float64)"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {},
   "execution_count": 20
  },
  {
   "source": [
    "b.mul_(2)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.6506, 1.5730, 1.4326],\n        [1.1346, 1.7720, 0.9185],\n        [0.4898, 1.3850, 0.4973],\n        [1.2708, 1.0569, 1.8689]], dtype=torch.float64)"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {},
   "execution_count": 21
  },
  {
   "source": [
    "a"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.65064969, 1.57301207, 1.43264714],\n       [1.13459218, 1.77197977, 0.91846448],\n       [0.48977678, 1.384985  , 0.49728394],\n       [1.27078262, 1.05688096, 1.86888358]])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {},
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}