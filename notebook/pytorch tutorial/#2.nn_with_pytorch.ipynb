{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.5.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python35264bitf8c6bb909d074297b47234dde81826f2",
   "display_name": "Python 3.5.2 64-bit"
  }
 },
 "cells": [
  {
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load dataset"
   ]
  },
  {
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                                ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('../data/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0it [00:00, ?it/s]Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n9920512it [00:03, 2994039.30it/s]\nExtracting ../data/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST_data/MNIST/raw\n0it [00:00, ?it/s]Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n32768it [00:00, 32951.98it/s]\n0it [00:00, ?it/s]Extracting ../data/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST_data/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n1654784it [00:01, 1376506.90it/s]\n0it [00:00, ?it/s]Extracting ../data/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST_data/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n8192it [00:00, 10919.22it/s]Extracting ../data/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST_data/MNIST/raw\nProcessing...\nDone!\n\n"
    }
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'torch.Tensor'>\ntorch.Size([64, 1, 28, 28])\ntorch.Size([64])\n"
    }
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r')"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f7c085aaf98>"
     },
     "metadata": {},
     "execution_count": 6
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      },
      "needs_background": "light"
     }
    }
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "source": [
    "def activation(x):\n",
    "    return 1 / (1+torch.exp(-x))"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define forward(except nn module)"
   ]
  },
  {
   "source": [
    "test = images[1]\n",
    "print(len(images))\n",
    "print(test.shape)\n",
    "torch.flatten(test).shape\n",
    "\n",
    "# input_img = torch.zeros(64, 784)\n",
    "\n",
    "# for img in range(len(images)):\n",
    "#     input_img[img] = torch.flatten(images[img])\n",
    "# print(input_img.shape[1])\n",
    "\n",
    "# flatten\n",
    "input_img = images.view(images.shape[0], -1)\n",
    "\n",
    "input_unit = input_img.shape[1]\n",
    "hidden_unit = 256\n",
    "output_unit = 10\n",
    "\n",
    "W1 = torch.rand(input_unit, hidden_unit)\n",
    "W2 = torch.rand(hidden_unit, output_unit)\n",
    "\n",
    "B1 = torch.rand(hidden_unit)\n",
    "B2 = torch.rand(output_unit)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "64\ntorch.Size([1, 28, 28])\n"
    }
   ],
   "metadata": {},
   "execution_count": 32
  },
  {
   "source": [
    "res_ith = activation(torch.mm(input_img, W1) + B1)\n",
    "res_hto = torch.mm(res_ith, W2) + B2"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 33
  },
  {
   "source": [
    "# res_hto"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 35
  },
  {
   "source": [
    "(torch.exp(res_hto).view(res_hto.shape[1], res_hto.shape[0]) / sum(torch.exp(res_hto).view(res_hto.shape[1], res_hto.shape[0]))).T"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317,\n         0.0704],\n        [0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059,\n         0.1388],\n        [0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135,\n         0.1025],\n        [0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868,\n         0.0542],\n        [0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704,\n         0.0819],\n        [0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388,\n         0.1144],\n        [0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025,\n         0.1317],\n        [0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542,\n         0.1059],\n        [0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819,\n         0.1135],\n        [0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144,\n         0.0868],\n        [0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317,\n         0.0704],\n        [0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059,\n         0.1388],\n        [0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135,\n         0.1025],\n        [0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868,\n         0.0542],\n        [0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704,\n         0.0819],\n        [0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388,\n         0.1144],\n        [0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025,\n         0.1317],\n        [0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542,\n         0.1059],\n        [0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819,\n         0.1135],\n        [0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144,\n         0.0868],\n        [0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317,\n         0.0704],\n        [0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059,\n         0.1388],\n        [0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135,\n         0.1025],\n        [0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868,\n         0.0542],\n        [0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704,\n         0.0819],\n        [0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388,\n         0.1144],\n        [0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025,\n         0.1317],\n        [0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542,\n         0.1059],\n        [0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819,\n         0.1135],\n        [0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144,\n         0.0868],\n        [0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317,\n         0.0704],\n        [0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059,\n         0.1388],\n        [0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135,\n         0.1025],\n        [0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868,\n         0.0542],\n        [0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704,\n         0.0819],\n        [0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388,\n         0.1144],\n        [0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025,\n         0.1317],\n        [0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542,\n         0.1059],\n        [0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819,\n         0.1135],\n        [0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144,\n         0.0868],\n        [0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317,\n         0.0704],\n        [0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059,\n         0.1388],\n        [0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135,\n         0.1025],\n        [0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868,\n         0.0542],\n        [0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704,\n         0.0819],\n        [0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388,\n         0.1144],\n        [0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025,\n         0.1317],\n        [0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542,\n         0.1059],\n        [0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819,\n         0.1135],\n        [0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144,\n         0.0868],\n        [0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317,\n         0.0704],\n        [0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059,\n         0.1388],\n        [0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135,\n         0.1025],\n        [0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868,\n         0.0542],\n        [0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704,\n         0.0819],\n        [0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388,\n         0.1144],\n        [0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025,\n         0.1317],\n        [0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542,\n         0.1059],\n        [0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819,\n         0.1135],\n        [0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144,\n         0.0868],\n        [0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317,\n         0.0704],\n        [0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059,\n         0.1388],\n        [0.1317, 0.0704, 0.0819, 0.1135, 0.1025, 0.1317, 0.0704, 0.0819, 0.1135,\n         0.1025],\n        [0.1059, 0.1388, 0.1144, 0.0868, 0.0542, 0.1059, 0.1388, 0.1144, 0.0868,\n         0.0542]])"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {},
   "execution_count": 61
  },
  {
   "source": [
    "sum(torch.exp(res_hto))"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([ 96.4839, 136.1699, 155.0400, 126.0674, 133.5946, 103.2539,  82.8882,\n        165.1745, 120.7021,  64.4550])"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {},
   "execution_count": 53
  },
  {
   "source": [
    "def softmax(x):\n",
    "    # x -> array of output units\n",
    "    return (torch.exp(x).T / sum(torch.exp(x).T)).T\n",
    "    # another return torch.exp(x) / sum(torch.exp(x), dim=1).view(-1, 1)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 62
  },
  {
   "source": [
    "res_hto = softmax(torch.mm(res_ith, W2) + B2)\n",
    "print(res_hto.shape)\n",
    "print(res_hto.sum(dim=1))"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([64, 10])\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
    }
   ],
   "metadata": {},
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define forward using nn module"
   ]
  },
  {
   "source": [
    "from torch import nn"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 66
  },
  {
   "source": [
    "# case1\n",
    "class Network_origin(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Network_origin2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer with sigmoid activation\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        # Output layer with softmax activation\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "source": [
    "# add 1 more hidden & changed activation ftn to Relu\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 128)\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden2 = nn.Linear(128, 64)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(64, 10)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 70
  },
  {
   "source": [
    "# Create the network and look at it's text representation\n",
    "model = Network()\n",
    "model"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Network(\n  (hidden): Linear(in_features=784, out_features=128, bias=True)\n  (hidden2): Linear(in_features=128, out_features=64, bias=True)\n  (output): Linear(in_features=64, out_features=10, bias=True)\n  (sigmoid): Sigmoid()\n  (relu): ReLU()\n  (softmax): Softmax(dim=1)\n)"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {},
   "execution_count": 71
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initializing weights and biases\n",
    "\n",
    "The weights and such are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with `model.fc1.weight` for instance."
   ]
  },
  {
   "source": [
    "print(model.hidden.weight)\n",
    "print(model.hidden.bias)\n",
    "\n",
    "# For custom initialization, we want to modify these tensors in place. These are actually autograd Variables, so we need to get back the actual tensors with model.fc1.weight.data. Once we have the tensors, we can fill them with zeros (for biases) or random normal values.\n",
    "\n",
    "# Set biases to all zeros\n",
    "model.hidden.bias.data.fill_(0)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Parameter containing:\ntensor([[ 0.0311, -0.0215,  0.0315,  ...,  0.0076,  0.0074,  0.0153],\n        [-0.0225,  0.0059, -0.0143,  ..., -0.0115, -0.0021,  0.0110],\n        [ 0.0250, -0.0225,  0.0254,  ...,  0.0242,  0.0191, -0.0198],\n        ...,\n        [-0.0334, -0.0299,  0.0113,  ..., -0.0239,  0.0354, -0.0165],\n        [ 0.0037,  0.0081,  0.0346,  ..., -0.0030, -0.0159,  0.0245],\n        [ 0.0132,  0.0026, -0.0333,  ..., -0.0192,  0.0331,  0.0016]],\n       requires_grad=True)\nParameter containing:\ntensor([ 0.0313, -0.0304, -0.0175,  0.0242, -0.0186, -0.0322,  0.0197, -0.0050,\n         0.0096,  0.0098,  0.0252,  0.0175,  0.0211, -0.0318,  0.0215,  0.0119,\n        -0.0266,  0.0240, -0.0023, -0.0049, -0.0350,  0.0296, -0.0160, -0.0330,\n         0.0230,  0.0256, -0.0058,  0.0159,  0.0249,  0.0182, -0.0098,  0.0274,\n         0.0228, -0.0310,  0.0343,  0.0160, -0.0249,  0.0024, -0.0087,  0.0310,\n         0.0230,  0.0219,  0.0134,  0.0268,  0.0315, -0.0047,  0.0263, -0.0211,\n         0.0225,  0.0295, -0.0064,  0.0027, -0.0117,  0.0244, -0.0321,  0.0168,\n        -0.0326, -0.0234,  0.0039, -0.0176,  0.0286, -0.0166,  0.0016,  0.0319,\n         0.0099,  0.0296,  0.0153, -0.0355,  0.0225, -0.0034, -0.0119,  0.0286,\n        -0.0289, -0.0291,  0.0206, -0.0216,  0.0257,  0.0089, -0.0021, -0.0272,\n         0.0299,  0.0136,  0.0142, -0.0298,  0.0200,  0.0313, -0.0038,  0.0336,\n         0.0271,  0.0285, -0.0322,  0.0014,  0.0294, -0.0239,  0.0246,  0.0182,\n         0.0093, -0.0332,  0.0064, -0.0281, -0.0128, -0.0291, -0.0032, -0.0298,\n         0.0282,  0.0093, -0.0134, -0.0172, -0.0134, -0.0147,  0.0149, -0.0175,\n         0.0005, -0.0011,  0.0070, -0.0230,  0.0258, -0.0290, -0.0064,  0.0029,\n        -0.0341, -0.0332, -0.0171, -0.0164, -0.0140, -0.0275, -0.0308,  0.0149],\n       requires_grad=True)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0.])"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {},
   "execution_count": 74
  },
  {
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "model.hidden.weight.data.normal_(std=0.01)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 0.0030, -0.0185, -0.0049,  ...,  0.0063,  0.0005, -0.0149],\n        [ 0.0062, -0.0048, -0.0202,  ...,  0.0036,  0.0065,  0.0179],\n        [-0.0018,  0.0032, -0.0126,  ..., -0.0090, -0.0075, -0.0081],\n        ...,\n        [-0.0092,  0.0094, -0.0084,  ...,  0.0140,  0.0125, -0.0088],\n        [ 0.0014, -0.0033,  0.0030,  ..., -0.0082, -0.0201, -0.0150],\n        [-0.0118,  0.0081,  0.0056,  ..., -0.0026,  0.0070,  0.0071]])"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {},
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Forward pass\n",
    "\n",
    "Now that we have a network, let's see what happens when we pass in an image."
   ]
  },
  {
   "source": [
    "# Grab some data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(64, 1, 784)\n",
    "# or images.resize_(images.shape[0], 1, 784) to automatically get batch size\n",
    "\n",
    "# Forward pass through the network\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])\n",
    "\n",
    "img = images[img_idx]\n",
    "# helper.view_classify(img.view(1, 28, 28), ps)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As you can see above, our network has basically no idea what this digit is. It's because we haven't trained it yet, all the weights are random!\n",
    "\n",
    "### Using `nn.Sequential`\n",
    "\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:"
   ]
  },
  {
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "# helper.view_classify(images[0].view(1, 28, 28), ps)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Sequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=10, bias=True)\n  (5): Softmax(dim=1)\n)\n"
    }
   ],
   "metadata": {},
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here our model is the same as before: 784 input units, a hidden layer with 128 units, ReLU activation, 64 unit hidden layer, another ReLU, then the output layer with 10 units, and the softmax output.\n",
    "\n",
    "The operations are available by passing in the appropriate index. For example, if you want to get first Linear operation and look at the weights, you'd use `model[0]`."
   ]
  },
  {
   "source": [
    "print(model[0])\n",
    "model[0].weight"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Linear(in_features=784, out_features=128, bias=True)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-0.0195,  0.0356,  0.0324,  ..., -0.0051,  0.0312,  0.0316],\n        [ 0.0256,  0.0122,  0.0030,  ...,  0.0101,  0.0196, -0.0284],\n        [ 0.0290, -0.0231, -0.0084,  ..., -0.0102, -0.0224,  0.0085],\n        ...,\n        [-0.0048,  0.0228,  0.0044,  ...,  0.0181, -0.0215, -0.0186],\n        [-0.0071,  0.0008, -0.0214,  ...,  0.0123,  0.0141,  0.0179],\n        [-0.0129, -0.0093, -0.0203,  ..., -0.0138,  0.0106,  0.0017]],\n       requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "metadata": {},
   "execution_count": 81
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can also pass in an `OrderedDict` to name the individual layers and operations, instead of using incremental integers. Note that dictionary keys must be unique, so _each operation must have a different name_."
   ]
  },
  {
   "source": [
    "from collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Sequential(\n  (fc1): Linear(in_features=784, out_features=128, bias=True)\n  (relu1): ReLU()\n  (fc2): Linear(in_features=128, out_features=64, bias=True)\n  (relu2): ReLU()\n  (output): Linear(in_features=64, out_features=10, bias=True)\n  (softmax): Softmax(dim=1)\n)"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "metadata": {},
   "execution_count": 82
  },
  {
   "source": [
    "# Now you can access layers either by integer or the name\n",
    "print(model[0])\n",
    "print(model.fc1)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Linear(in_features=784, out_features=128, bias=True)\nLinear(in_features=784, out_features=128, bias=True)\n"
    }
   ],
   "metadata": {},
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}